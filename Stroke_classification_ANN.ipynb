{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf2ec714-06ff-4b99-8c72-b008cef020b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>Residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>Male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>Female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>202.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>Male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Rural</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>Female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Private</td>\n",
       "      <td>Urban</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>smokes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>Female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>Rural</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
       "0   9046    Male  67.0             0              1          Yes   \n",
       "1  51676  Female  61.0             0              0          Yes   \n",
       "2  31112    Male  80.0             0              1          Yes   \n",
       "3  60182  Female  49.0             0              0          Yes   \n",
       "4   1665  Female  79.0             1              0          Yes   \n",
       "\n",
       "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
       "0        Private          Urban             228.69  36.6  formerly smoked   \n",
       "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
       "2        Private          Rural             105.92  32.5     never smoked   \n",
       "3        Private          Urban             171.23  34.4           smokes   \n",
       "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
       "\n",
       "   stroke  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'healthcare-dataset-stroke-data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe1e121e-c39c-4684-b0fe-8b8f62572615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5110 entries, 0 to 5109\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 5110 non-null   int64  \n",
      " 1   gender             5110 non-null   int64  \n",
      " 2   age                5110 non-null   float64\n",
      " 3   hypertension       5110 non-null   int64  \n",
      " 4   heart_disease      5110 non-null   int64  \n",
      " 5   ever_married       5110 non-null   int64  \n",
      " 6   work_type          5110 non-null   int64  \n",
      " 7   Residence_type     5110 non-null   int64  \n",
      " 8   avg_glucose_level  5110 non-null   float64\n",
      " 9   bmi                5110 non-null   float64\n",
      " 10  smoking_status     5110 non-null   int64  \n",
      " 11  stroke             5110 non-null   int64  \n",
      "dtypes: float64(3), int64(9)\n",
      "memory usage: 479.2 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "       id  gender   age  hypertension  heart_disease  ever_married  work_type  \\\n",
       " 0   9046       1  67.0             0              1             1          2   \n",
       " 1  51676       0  61.0             0              0             1          3   \n",
       " 2  31112       1  80.0             0              1             1          2   \n",
       " 3  60182       0  49.0             0              0             1          2   \n",
       " 4   1665       0  79.0             1              0             1          3   \n",
       " \n",
       "    Residence_type  avg_glucose_level        bmi  smoking_status  stroke  \n",
       " 0               1             228.69  36.600000               1       1  \n",
       " 1               0             202.21  28.893237               2       1  \n",
       " 2               0             105.92  32.500000               2       1  \n",
       " 3               1             171.23  34.400000               3       1  \n",
       " 4               0             174.12  24.000000               2       1  )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Checking for missing values in the dataset\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Impute missing values for 'bmi' using the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data['bmi'] = imputer.fit_transform(data[['bmi']])\n",
    "\n",
    "# Encode categorical columns using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# Display the cleaned dataset summary\n",
    "data.info(), data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "152f9ebb-20fb-4ed1-a255-f134b105d14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk_level\n",
       "0    3895\n",
       "2    1114\n",
       "1     101\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining risk levels based on specific criteria\n",
    "def risk_level(row):\n",
    "    # High risk: stroke, high glucose level, and presence of hypertension or heart disease\n",
    "    if row['stroke'] == 1 or row['avg_glucose_level'] > 200 or (row['hypertension'] == 1 or row['heart_disease'] == 1):\n",
    "        return 2  # High risk\n",
    "    # Medium risk: moderately high glucose level and age above 50\n",
    "    elif row['avg_glucose_level'] > 140 and row['age'] > 50:\n",
    "        return 1  # Medium risk\n",
    "    else:\n",
    "        return 0  # Low risk\n",
    "\n",
    "# Apply the function to create the 'risk_level' column\n",
    "data['risk_level'] = data.apply(risk_level, axis=1)\n",
    "\n",
    "# Drop the 'stroke' column as we now have 'risk_level' for classification\n",
    "data = data.drop(columns=['stroke', 'id'])\n",
    "\n",
    "# Display the distribution of risk levels\n",
    "risk_distribution = data['risk_level'].value_counts()\n",
    "risk_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b239a9a4-5622-432a-8577-718257b2e5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk_level\n",
       "0    4002\n",
       "2     989\n",
       "1     119\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefining risk levels based on available columns\n",
    "def risk_level(row):\n",
    "    # High risk: high glucose level or presence of hypertension or heart disease\n",
    "    if row['avg_glucose_level'] > 200 or (row['hypertension'] == 1 or row['heart_disease'] == 1):\n",
    "        return 2  # High risk\n",
    "    # Medium risk: moderately high glucose level and age above 50\n",
    "    elif row['avg_glucose_level'] > 140 and row['age'] > 50:\n",
    "        return 1  # Medium risk\n",
    "    else:\n",
    "        return 0  # Low risk\n",
    "\n",
    "# Apply the function to create the 'risk_level' column\n",
    "data['risk_level'] = data.apply(risk_level, axis=1)\n",
    "\n",
    "# Display the updated distribution of risk levels\n",
    "risk_distribution = data['risk_level'].value_counts()\n",
    "risk_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8fadedc-011e-4d34-9d8c-126ed7502fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4088, 10), (1022, 10), (4088,), (1022,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Features and target variable\n",
    "X = data.drop('risk_level', axis=1)\n",
    "y = data['risk_level']\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Display the shape of the training and testing sets\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "971f639c-f9b3-41c5-aa87-8aaac8472e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - accuracy: 0.8046 - loss: 0.5847 - val_accuracy: 0.9719 - val_loss: 0.1102\n",
      "Epoch 2/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - accuracy: 0.9692 - loss: 0.0943 - val_accuracy: 0.9731 - val_loss: 0.0692\n",
      "Epoch 3/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363us/step - accuracy: 0.9725 - loss: 0.0691 - val_accuracy: 0.9756 - val_loss: 0.0565\n",
      "Epoch 4/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357us/step - accuracy: 0.9715 - loss: 0.0637 - val_accuracy: 0.9804 - val_loss: 0.0491\n",
      "Epoch 5/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - accuracy: 0.9815 - loss: 0.0424 - val_accuracy: 0.9804 - val_loss: 0.0434\n",
      "Epoch 6/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - accuracy: 0.9856 - loss: 0.0391 - val_accuracy: 0.9804 - val_loss: 0.0442\n",
      "Epoch 7/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - accuracy: 0.9883 - loss: 0.0311 - val_accuracy: 0.9829 - val_loss: 0.0382\n",
      "Epoch 8/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.9916 - loss: 0.0343 - val_accuracy: 0.9853 - val_loss: 0.0409\n",
      "Epoch 9/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - accuracy: 0.9896 - loss: 0.0309 - val_accuracy: 0.9853 - val_loss: 0.0362\n",
      "Epoch 10/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - accuracy: 0.9952 - loss: 0.0209 - val_accuracy: 0.9841 - val_loss: 0.0389\n",
      "Epoch 11/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - accuracy: 0.9935 - loss: 0.0248 - val_accuracy: 0.9841 - val_loss: 0.0375\n",
      "Epoch 12/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - accuracy: 0.9942 - loss: 0.0184 - val_accuracy: 0.9878 - val_loss: 0.0361\n",
      "Epoch 13/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344us/step - accuracy: 0.9943 - loss: 0.0193 - val_accuracy: 0.9866 - val_loss: 0.0346\n",
      "Epoch 14/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - accuracy: 0.9961 - loss: 0.0140 - val_accuracy: 0.9829 - val_loss: 0.0439\n",
      "Epoch 15/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - accuracy: 0.9897 - loss: 0.0251 - val_accuracy: 0.9866 - val_loss: 0.0350\n",
      "Epoch 16/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.9937 - loss: 0.0213 - val_accuracy: 0.9853 - val_loss: 0.0303\n",
      "Epoch 17/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - accuracy: 0.9977 - loss: 0.0106 - val_accuracy: 0.9890 - val_loss: 0.0315\n",
      "Epoch 18/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - accuracy: 0.9946 - loss: 0.0117 - val_accuracy: 0.9866 - val_loss: 0.0302\n",
      "Epoch 19/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.9988 - loss: 0.0095 - val_accuracy: 0.9890 - val_loss: 0.0297\n",
      "Epoch 20/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - accuracy: 0.9941 - loss: 0.0121 - val_accuracy: 0.9878 - val_loss: 0.0325\n",
      "Epoch 21/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - accuracy: 0.9969 - loss: 0.0106 - val_accuracy: 0.9878 - val_loss: 0.0329\n",
      "Epoch 22/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - accuracy: 0.9964 - loss: 0.0113 - val_accuracy: 0.9866 - val_loss: 0.0287\n",
      "Epoch 23/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - accuracy: 0.9969 - loss: 0.0082 - val_accuracy: 0.9878 - val_loss: 0.0322\n",
      "Epoch 24/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - accuracy: 0.9992 - loss: 0.0059 - val_accuracy: 0.9878 - val_loss: 0.0258\n",
      "Epoch 25/25\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.9993 - loss: 0.0059 - val_accuracy: 0.9902 - val_loss: 0.0258\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert target variable to categorical (one-hot encoding for multiclass classification)\n",
    "y_train_categorical = to_categorical(y_train, num_classes=3)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "# Building the ANN model\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # Output layer for 3 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_categorical, validation_split=0.2, epochs=25, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8b6c857-bd52-4bbd-b848-2a95cf154413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.02%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_categorical, verbose=0)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82621af1-fb55-4356-9cd3-a0815a83fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save('risk_stratification_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3487d054-3c1f-41d5-819e-240a0a7bac36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scaler.pkl'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the fitted scaler to a file for deployment purposes\n",
    "scaler_filename = 'scaler.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "scaler_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "210b1f6b-320a-4e38-a27c-a0ee0d94c4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug: * Restarting with watchdog (fsevents)\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n",
      "    app.initialize(argv)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 118, in inner\n",
      "    return method(app, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 654, in initialize\n",
      "    self.init_sockets()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 331, in init_sockets\n",
      "    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 253, in _bind_socket\n",
      "    return self._try_bind_socket(s, port)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 229, in _try_bind_socket\n",
      "    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/zmq/sugar/socket.py\", line 302, in bind\n",
      "    super().bind(addr)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 564, in zmq.backend.cython.socket.Socket.bind\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 28, in zmq.backend.cython.checkrc._check_rc\n",
      "zmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:51431')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('risk_stratification_model.h5')\n",
    "\n",
    "# Load the scaler used for preprocessing (save it using joblib after fitting)\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define the API endpoint for prediction\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Get the JSON data from the request\n",
    "        data = request.json\n",
    "        # Extract the features from the JSON\n",
    "        features = np.array([[\n",
    "            data['gender'],\n",
    "            data['age'],\n",
    "            data['hypertension'],\n",
    "            data['heart_disease'],\n",
    "            data['ever_married'],\n",
    "            data['work_type'],\n",
    "            data['Residence_type'],\n",
    "            data['avg_glucose_level'],\n",
    "            data['bmi'],\n",
    "            data['smoking_status']\n",
    "        ]])\n",
    "        \n",
    "        # Scale the features using the pre-fitted scaler\n",
    "        features_scaled = scaler.transform(features)\n",
    "        \n",
    "        # Predict the risk level\n",
    "        prediction = model.predict(features_scaled)\n",
    "        risk_level = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "        # Map the risk level to a string response\n",
    "        risk_mapping = {0: 'Low Risk', 1: 'Medium Risk', 2: 'High Risk'}\n",
    "        risk_str = risk_mapping[risk_level]\n",
    "\n",
    "        return jsonify({'risk_level': risk_str})\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d026c4a8-8eaf-40f0-845f-c712a5a374af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Fit the scaler on the training data and save it\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f9cd31e-261c-4b1f-a457-b0efaa8324c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Predicted Risk Level: High Risk\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('risk_stratification_model.h5')\n",
    "\n",
    "# Load the scaler\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Define a test input (ensure it matches the format of your training data)\n",
    "test_input = np.array([[\n",
    "    1,    # gender (encoded as integer)\n",
    "    65.0, # age\n",
    "    1,    # hypertension\n",
    "    0,    # heart_disease\n",
    "    1,    # ever_married\n",
    "    2,    # work_type\n",
    "    1,    # Residence_type\n",
    "    180.0, # avg_glucose_level\n",
    "    32.5,  # bmi\n",
    "    2     # smoking_status\n",
    "]])\n",
    "\n",
    "# Scale the test input\n",
    "test_input_scaled = scaler.transform(test_input)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(test_input_scaled)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "# Map the class to the risk level\n",
    "risk_mapping = {0: 'Low Risk', 1: 'Medium Risk', 2: 'High Risk'}\n",
    "risk_str = risk_mapping[predicted_class]\n",
    "\n",
    "print(f'Predicted Risk Level: {risk_str}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "849cb6e5-0132-44fd-9ed3-a3e4899e3cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Sample 1: Predicted Risk Level - High Risk\n",
      "Sample 2: Predicted Risk Level - High Risk\n"
     ]
    }
   ],
   "source": [
    "test_inputs = np.array([\n",
    "    [1, 65.0, 1, 0, 1, 2, 1, 180.0, 32.5, 2],  # Sample 1\n",
    "    [0, 45.0, 0, 0, 0, 1, 0, 130.0, 25.0, 0]   # Sample 2\n",
    "])\n",
    "\n",
    "# Scale the test inputs\n",
    "test_inputs_scaled = scaler.transform(test_inputs)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_inputs_scaled)\n",
    "\n",
    "# Convert predictions to risk levels\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "risk_levels = [risk_mapping[cls] for cls in predicted_classes]\n",
    "\n",
    "for i, risk in enumerate(risk_levels):\n",
    "    print(f'Sample {i+1}: Predicted Risk Level - {risk}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31499d9a-91af-4a7a-a3d3-23fb2ccc5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low Risk Input\n",
    "low_risk_input = np.array([[\n",
    "    0,    # gender (Female)\n",
    "    30.0, # age\n",
    "    0,    # hypertension\n",
    "    0,    # heart_disease\n",
    "    0,    # ever_married\n",
    "    0,    # work_type\n",
    "    0,    # Residence_type\n",
    "    80, # avg_glucose_level\n",
    "    22.0,  # bmi\n",
    "    0     # smoking_status\n",
    "]])\n",
    "\n",
    "# Medium Risk Input\n",
    "medium_risk_input = np.array([[\n",
    "    1,    # gender (Male)\n",
    "    60.0, # age\n",
    "    0,    # hypertension\n",
    "    0,    # heart_disease\n",
    "    1,    # ever_married\n",
    "    2,    # work_type\n",
    "    1,    # Residence_type\n",
    "    160.0, # avg_glucose_level\n",
    "    28.0,  # bmi\n",
    "    2     # smoking_status\n",
    "]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d4c47b8-4328-48fa-9cd1-1ed20cc7aa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Low Risk Sample: Predicted Risk Level - High Risk\n",
      "Medium Risk Sample: Predicted Risk Level - High Risk\n"
     ]
    }
   ],
   "source": [
    "# Scale the low and medium risk inputs\n",
    "low_risk_input_scaled = scaler.transform(low_risk_input)\n",
    "medium_risk_input_scaled = scaler.transform(medium_risk_input)\n",
    "\n",
    "# Make predictions\n",
    "low_risk_prediction = model.predict(low_risk_input_scaled)\n",
    "medium_risk_prediction = model.predict(medium_risk_input_scaled)\n",
    "\n",
    "# Get the predicted classes\n",
    "low_risk_class = np.argmax(low_risk_prediction, axis=1)[0]\n",
    "medium_risk_class = np.argmax(medium_risk_prediction, axis=1)[0]\n",
    "\n",
    "# Map the classes to risk levels\n",
    "low_risk_str = risk_mapping[low_risk_class]\n",
    "medium_risk_str = risk_mapping[medium_risk_class]\n",
    "\n",
    "print(f'Low Risk Sample: Predicted Risk Level - {low_risk_str}')\n",
    "print(f'Medium Risk Sample: Predicted Risk Level - {medium_risk_str}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f86fc6f-8798-4e1a-8b68-42feb92336f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jn/7q4k5xqj6tdc7t0t0_c17hrm0000gn/T/ipykernel_4812/2403954135.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['bmi'].fillna(data['bmi'].median(), inplace=True)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - accuracy: 0.3381 - loss: 0.9356 - val_accuracy: 0.8301 - val_loss: 0.5981\n",
      "Epoch 2/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - accuracy: 0.8915 - loss: 0.4407 - val_accuracy: 0.9389 - val_loss: 0.1934\n",
      "Epoch 3/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - accuracy: 0.9464 - loss: 0.2222 - val_accuracy: 0.9584 - val_loss: 0.1262\n",
      "Epoch 4/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339us/step - accuracy: 0.9665 - loss: 0.1626 - val_accuracy: 0.9548 - val_loss: 0.1166\n",
      "Epoch 5/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - accuracy: 0.9597 - loss: 0.1331 - val_accuracy: 0.9670 - val_loss: 0.0788\n",
      "Epoch 6/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340us/step - accuracy: 0.9716 - loss: 0.0923 - val_accuracy: 0.9621 - val_loss: 0.0756\n",
      "Epoch 7/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361us/step - accuracy: 0.9720 - loss: 0.0913 - val_accuracy: 0.9658 - val_loss: 0.0619\n",
      "Epoch 8/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - accuracy: 0.9765 - loss: 0.0831 - val_accuracy: 0.9658 - val_loss: 0.0639\n",
      "Epoch 9/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343us/step - accuracy: 0.9790 - loss: 0.0635 - val_accuracy: 0.9694 - val_loss: 0.0602\n",
      "Epoch 10/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - accuracy: 0.9821 - loss: 0.0642 - val_accuracy: 0.9719 - val_loss: 0.0556\n",
      "Epoch 11/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360us/step - accuracy: 0.9827 - loss: 0.0492 - val_accuracy: 0.9743 - val_loss: 0.0507\n",
      "Epoch 12/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - accuracy: 0.9861 - loss: 0.0509 - val_accuracy: 0.9817 - val_loss: 0.0410\n",
      "Epoch 13/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - accuracy: 0.9817 - loss: 0.0450 - val_accuracy: 0.9768 - val_loss: 0.0577\n",
      "Epoch 14/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347us/step - accuracy: 0.9873 - loss: 0.0348 - val_accuracy: 0.9829 - val_loss: 0.0414\n",
      "Epoch 15/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.9877 - loss: 0.0414 - val_accuracy: 0.9829 - val_loss: 0.0431\n",
      "Epoch 16/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - accuracy: 0.9854 - loss: 0.0648 - val_accuracy: 0.9829 - val_loss: 0.0495\n",
      "Epoch 17/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - accuracy: 0.9901 - loss: 0.0336 - val_accuracy: 0.9804 - val_loss: 0.0438\n",
      "Epoch 18/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - accuracy: 0.9898 - loss: 0.0217 - val_accuracy: 0.9792 - val_loss: 0.0635\n",
      "Epoch 19/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355us/step - accuracy: 0.9871 - loss: 0.0444 - val_accuracy: 0.9878 - val_loss: 0.0307\n",
      "Epoch 20/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - accuracy: 0.9941 - loss: 0.0208 - val_accuracy: 0.9817 - val_loss: 0.0499\n",
      "Epoch 21/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348us/step - accuracy: 0.9940 - loss: 0.0233 - val_accuracy: 0.9866 - val_loss: 0.0310\n",
      "Epoch 22/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - accuracy: 0.9943 - loss: 0.0189 - val_accuracy: 0.9902 - val_loss: 0.0296\n",
      "Epoch 23/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - accuracy: 0.9944 - loss: 0.0127 - val_accuracy: 0.9853 - val_loss: 0.0364\n",
      "Epoch 24/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345us/step - accuracy: 0.9933 - loss: 0.0282 - val_accuracy: 0.9902 - val_loss: 0.0364\n",
      "Epoch 25/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.9929 - loss: 0.0262 - val_accuracy: 0.9829 - val_loss: 0.0459\n",
      "Epoch 26/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368us/step - accuracy: 0.9916 - loss: 0.0345 - val_accuracy: 0.9902 - val_loss: 0.0298\n",
      "Epoch 27/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - accuracy: 0.9953 - loss: 0.0145 - val_accuracy: 0.9902 - val_loss: 0.0333\n",
      "Epoch 28/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - accuracy: 0.9929 - loss: 0.0190 - val_accuracy: 0.9902 - val_loss: 0.0325\n",
      "Epoch 29/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365us/step - accuracy: 0.9949 - loss: 0.0223 - val_accuracy: 0.9866 - val_loss: 0.0343\n",
      "Epoch 30/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 364us/step - accuracy: 0.9956 - loss: 0.0209 - val_accuracy: 0.9890 - val_loss: 0.0286\n",
      "Epoch 31/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - accuracy: 0.9972 - loss: 0.0099 - val_accuracy: 0.9902 - val_loss: 0.0300\n",
      "Epoch 32/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - accuracy: 0.9972 - loss: 0.0115 - val_accuracy: 0.9841 - val_loss: 0.0703\n",
      "Epoch 33/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382us/step - accuracy: 0.9938 - loss: 0.0244 - val_accuracy: 0.9927 - val_loss: 0.0268\n",
      "Epoch 34/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358us/step - accuracy: 0.9985 - loss: 0.0080 - val_accuracy: 0.9902 - val_loss: 0.0271\n",
      "Epoch 35/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - accuracy: 0.9989 - loss: 0.0071 - val_accuracy: 0.9914 - val_loss: 0.0334\n",
      "Epoch 36/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359us/step - accuracy: 0.9964 - loss: 0.0080 - val_accuracy: 0.9914 - val_loss: 0.0304\n",
      "Epoch 37/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.9954 - loss: 0.0072 - val_accuracy: 0.9890 - val_loss: 0.0315\n",
      "Epoch 38/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - accuracy: 0.9968 - loss: 0.0104 - val_accuracy: 0.9914 - val_loss: 0.0315\n",
      "Epoch 39/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353us/step - accuracy: 0.9994 - loss: 0.0056 - val_accuracy: 0.9914 - val_loss: 0.0331\n",
      "Epoch 40/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - accuracy: 0.9977 - loss: 0.0054 - val_accuracy: 0.9927 - val_loss: 0.0353\n",
      "Epoch 41/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356us/step - accuracy: 0.9967 - loss: 0.0148 - val_accuracy: 0.9878 - val_loss: 0.0530\n",
      "Epoch 42/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354us/step - accuracy: 0.9957 - loss: 0.0229 - val_accuracy: 0.9890 - val_loss: 0.0465\n",
      "Epoch 43/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 349us/step - accuracy: 0.9990 - loss: 0.0050 - val_accuracy: 0.9829 - val_loss: 0.1119\n",
      "Epoch 44/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step - accuracy: 0.9892 - loss: 0.0956 - val_accuracy: 0.9792 - val_loss: 0.0632\n",
      "Epoch 45/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.9922 - loss: 0.0305 - val_accuracy: 0.9829 - val_loss: 0.0641\n",
      "Epoch 46/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352us/step - accuracy: 0.9916 - loss: 0.0594 - val_accuracy: 0.9890 - val_loss: 0.0424\n",
      "Epoch 47/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441us/step - accuracy: 0.9954 - loss: 0.0216 - val_accuracy: 0.9914 - val_loss: 0.0309\n",
      "Epoch 48/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357us/step - accuracy: 0.9910 - loss: 0.1097 - val_accuracy: 0.9890 - val_loss: 0.0469\n",
      "Epoch 49/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351us/step - accuracy: 0.9961 - loss: 0.0179 - val_accuracy: 0.9927 - val_loss: 0.0285\n",
      "Epoch 50/50\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342us/step - accuracy: 0.9977 - loss: 0.0062 - val_accuracy: 0.9890 - val_loss: 0.0328\n",
      "Test Accuracy: 98.53%\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[791   5   4]\n",
      " [  1  21   2]\n",
      " [  2   1 195]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Low Risk       1.00      0.99      0.99       800\n",
      " Medium Risk       0.78      0.88      0.82        24\n",
      "   High Risk       0.97      0.98      0.98       198\n",
      "\n",
      "    accuracy                           0.99      1022\n",
      "   macro avg       0.91      0.95      0.93      1022\n",
      "weighted avg       0.99      0.99      0.99      1022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('healthcare-dataset-stroke-data.csv')  # Update the path\n",
    "\n",
    "# Impute missing values for 'bmi' using the median\n",
    "data['bmi'].fillna(data['bmi'].median(), inplace=True)\n",
    "\n",
    "# Encode categorical columns using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n",
    "for col in categorical_columns:\n",
    "    data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# Define the risk levels based on the criteria\n",
    "def risk_level(row):\n",
    "    if row['avg_glucose_level'] > 200 or (row['hypertension'] == 1 or row['heart_disease'] == 1):\n",
    "        return 2  # High risk\n",
    "    elif row['avg_glucose_level'] > 140 and row['age'] > 50:\n",
    "        return 1  # Medium risk\n",
    "    else:\n",
    "        return 0  # Low risk\n",
    "\n",
    "data['risk_level'] = data.apply(risk_level, axis=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['id'])\n",
    "\n",
    "# Features and target variable\n",
    "X = data.drop('risk_level', axis=1)\n",
    "y = data['risk_level']\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for deployment\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Convert target variable to categorical (one-hot encoding for multiclass classification)\n",
    "y_train_categorical = to_categorical(y_train, num_classes=3)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "# Calculate class weights for handling imbalanced data\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Building the ANN model\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # Output layer for 3 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(\n",
    "    X_train, y_train_categorical, \n",
    "    validation_split=0.2, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    class_weight=class_weights_dict, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_categorical, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Print confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred_classes))\n",
    "print(classification_report(y_test, y_pred_classes, target_names=['Low Risk', 'Medium Risk', 'High Risk']))\n",
    "\n",
    "# Save the trained model for deployment\n",
    "model.save('risk_stratification_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fc96960-8f45-4608-aeb4-e5a8e9469db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low Risk Input\n",
    "low_risk_input = np.array([[\n",
    "    0,    # gender (Female)\n",
    "    30.0, # age\n",
    "    0,    # hypertension\n",
    "    0,    # heart_disease\n",
    "    0,    # ever_married\n",
    "    1,    # work_type\n",
    "    0,    # Residence_type\n",
    "    120.0, # avg_glucose_level\n",
    "    22.0,  # bmi\n",
    "    1     # smoking_status\n",
    "]])\n",
    "\n",
    "# Medium Risk Input\n",
    "medium_risk_input = np.array([[\n",
    "    1,    # gender (Male)\n",
    "    60.0, # age\n",
    "    0,    # hypertension\n",
    "    0,    # heart_disease\n",
    "    1,    # ever_married\n",
    "    2,    # work_type\n",
    "    1,    # Residence_type\n",
    "    160.0, # avg_glucose_level\n",
    "    28.0,  # bmi\n",
    "    2     # smoking_status\n",
    "]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b040e1a-4986-4eb8-b14e-5d5e74ab9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Low Risk Sample: Predicted Risk Level - Low Risk\n",
      "Medium Risk Sample: Predicted Risk Level - Medium Risk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model = tf.keras.models.load_model('risk_stratification_model.h5')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Define sample inputs for low and medium risk\n",
    "# Ensure all 11 features are present as used during training\n",
    "low_risk_input = np.array([\n",
    "    [0, 30.0, 0, 0, 0, 1, 0, 120.0, 22.0, 1, 0]  # Low risk sample, added placeholder for the 11th feature\n",
    "])\n",
    "\n",
    "medium_risk_input = np.array([\n",
    "    [1, 60.0, 0, 0, 1, 2, 1, 160.0, 28.0, 2, 0]  # Medium risk sample, added placeholder for the 11th feature\n",
    "])\n",
    "\n",
    "# Scale the low and medium risk inputs\n",
    "low_risk_input_scaled = scaler.transform(low_risk_input)\n",
    "medium_risk_input_scaled = scaler.transform(medium_risk_input)\n",
    "\n",
    "# Make predictions\n",
    "low_risk_prediction = model.predict(low_risk_input_scaled)\n",
    "medium_risk_prediction = model.predict(medium_risk_input_scaled)\n",
    "\n",
    "# Get the predicted classes\n",
    "low_risk_class = np.argmax(low_risk_prediction, axis=1)[0]\n",
    "medium_risk_class = np.argmax(medium_risk_prediction, axis=1)[0]\n",
    "\n",
    "# Map the classes to risk levels\n",
    "risk_mapping = {0: 'Low Risk', 1: 'Medium Risk', 2: 'High Risk'}\n",
    "low_risk_str = risk_mapping[low_risk_class]\n",
    "medium_risk_str = risk_mapping[medium_risk_class]\n",
    "\n",
    "print(f'Low Risk Sample: Predicted Risk Level - {low_risk_str}')\n",
    "print(f'Medium Risk Sample: Predicted Risk Level - {medium_risk_str}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1718760-53a5-421b-8c86-d85e8c9c61a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Low Risk Sample: Predicted Risk Level - Low Risk\n",
      "Medium Risk Sample: Predicted Risk Level - Medium Risk\n",
      "High Risk Sample: Predicted Risk Level - High Risk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model = tf.keras.models.load_model('risk_stratification_model.h5')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Define sample inputs for low, medium, and high risk\n",
    "low_risk_input = np.array([\n",
    "    [0, 30.0, 0, 0, 0, 1, 0, 120.0, 22.0, 1, 0]  # Low risk sample\n",
    "])\n",
    "\n",
    "medium_risk_input = np.array([\n",
    "    [1, 60.0, 0, 0, 1, 2, 1, 160.0, 28.0, 2, 0]  # Medium risk sample\n",
    "])\n",
    "\n",
    "high_risk_input = np.array([\n",
    "    [1, 70.0, 1, 1, 1, 3, 1, 250.0, 35.0, 3, 0]  # High risk sample\n",
    "])\n",
    "\n",
    "# Scale the inputs\n",
    "low_risk_input_scaled = scaler.transform(low_risk_input)\n",
    "medium_risk_input_scaled = scaler.transform(medium_risk_input)\n",
    "high_risk_input_scaled = scaler.transform(high_risk_input)\n",
    "\n",
    "# Make predictions\n",
    "low_risk_prediction = model.predict(low_risk_input_scaled)\n",
    "medium_risk_prediction = model.predict(medium_risk_input_scaled)\n",
    "high_risk_prediction = model.predict(high_risk_input_scaled)\n",
    "\n",
    "# Get the predicted classes\n",
    "low_risk_class = np.argmax(low_risk_prediction, axis=1)[0]\n",
    "medium_risk_class = np.argmax(medium_risk_prediction, axis=1)[0]\n",
    "high_risk_class = np.argmax(high_risk_prediction, axis=1)[0]\n",
    "\n",
    "# Map the classes to risk levels\n",
    "risk_mapping = {0: 'Low Risk', 1: 'Medium Risk', 2: 'High Risk'}\n",
    "low_risk_str = risk_mapping[low_risk_class]\n",
    "medium_risk_str = risk_mapping[medium_risk_class]\n",
    "high_risk_str = risk_mapping[high_risk_class]\n",
    "\n",
    "print(f'Low Risk Sample: Predicted Risk Level - {low_risk_str}')\n",
    "print(f'Medium Risk Sample: Predicted Risk Level - {medium_risk_str}')\n",
    "print(f'High Risk Sample: Predicted Risk Level - {high_risk_str}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74d6ebb1-6dac-4a8e-adc5-b7b4f4bf5e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 38 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3166b7100> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "WARNING:tensorflow:5 out of the last 38 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3166b7100> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Sample 1: Predicted Risk Level - Low Risk\n",
      "Sample 2: Predicted Risk Level - Low Risk\n",
      "Sample 3: Predicted Risk Level - Medium Risk\n",
      "Sample 4: Predicted Risk Level - Medium Risk\n",
      "Sample 5: Predicted Risk Level - High Risk\n",
      "Sample 6: Predicted Risk Level - High Risk\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model = tf.keras.models.load_model('risk_stratification_model.h5')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Define multiple sample inputs for different risk levels\n",
    "sample_inputs = np.array([\n",
    "    [0, 45.0, 0, 0, 1, 0, 1, 100.0, 24.0, 0, 0],  # Low Risk Sample 2\n",
    "    [1, 25.0, 0, 0, 0, 4, 0, 110.0, 27.5, 1, 0],  # Low Risk Sample 3\n",
    "    [0, 55.0, 0, 0, 1, 2, 1, 170.0, 30.0, 2, 0],  # Medium Risk Sample 2\n",
    "    [1, 62.0, 0, 0, 1, 3, 0, 150.0, 26.0, 3, 0],  # Medium Risk Sample 3\n",
    "    [0, 75.0, 1, 0, 1, 1, 1, 220.0, 33.0, 3, 0],  # High Risk Sample 2\n",
    "    [1, 55.0, 1, 1, 1, 3, 0, 210.0, 31.0, 2, 0],  # High Risk Sample 3\n",
    "])\n",
    "\n",
    "# Scale the inputs\n",
    "sample_inputs_scaled = scaler.transform(sample_inputs)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(sample_inputs_scaled)\n",
    "\n",
    "# Get the predicted classes\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map the classes to risk levels\n",
    "risk_mapping = {0: 'Low Risk', 1: 'Medium Risk', 2: 'High Risk'}\n",
    "predicted_risks = [risk_mapping[cls] for cls in predicted_classes]\n",
    "\n",
    "# Print the predicted risk levels for each sample\n",
    "for i, risk in enumerate(predicted_risks):\n",
    "    print(f'Sample {i+1}: Predicted Risk Level - {risk}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb4b8ca5-fb29-4001-af31-0668f453fce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Sample 1: Predicted Risk Level - High Risk\n",
      "Sample 2: Predicted Risk Level - Medium Risk\n",
      "Sample 3: Predicted Risk Level - High Risk\n",
      "Sample 4: Predicted Risk Level - Medium Risk\n",
      "Sample 5: Predicted Risk Level - Medium Risk\n",
      "Sample 6: Predicted Risk Level - Low Risk\n",
      "Sample 7: Predicted Risk Level - High Risk\n",
      "Sample 8: Predicted Risk Level - High Risk\n",
      "Sample 9: Predicted Risk Level - Low Risk\n",
      "Sample 10: Predicted Risk Level - High Risk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# Load the trained model and scaler\n",
    "model = tf.keras.models.load_model('risk_stratification_model.h5')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Define multiple sample inputs for different risk levels\n",
    "sample_inputs = np.array([\n",
    "    [1, 66.0, 1, 0, 1, 4, 0, 225.0, 35.0, 3, 0], # High Risk Sample 6\n",
    "    [1, 52.0, 0, 0, 1, 0, 1, 145.0, 28.0, 3, 0], # Medium Risk Sample 6\n",
    "    [1, 50.0, 1, 1, 1, 3, 1, 200.0, 34.0, 2, 0], # High Risk Sample 5\n",
    "    [1, 58.0, 0, 0, 1, 2, 1, 165.0, 29.0, 2, 0], # Medium Risk Sample 4\n",
    "    [0, 64.0, 0, 0, 1, 3, 0, 155.0, 32.0, 3, 0], # Medium Risk Sample 5\n",
    "    [0, 28.0, 0, 0, 0, 1, 0, 90.0, 21.0, 1, 0],  # Low Risk Sample 4\n",
    "    [0, 78.0, 1, 0, 1, 1, 0, 210.0, 30.0, 2, 0], # High Risk Sample 4\n",
    "    [1, 50.0, 1, 1, 1, 3, 1, 200.0, 34.0, 2, 0], # High Risk Sample 5\n",
    "    [1, 22.0, 0, 0, 0, 3, 1, 105.0, 23.0, 1, 0], # Low Risk Sample 5\n",
    "    [0, 72.0, 1, 1, 1, 2, 1, 250.0, 37.0, 2, 0], # High Risk Sample 7\n",
    "])\n",
    "\n",
    "# Scale the inputs\n",
    "sample_inputs_scaled = scaler.transform(sample_inputs)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(sample_inputs_scaled)\n",
    "\n",
    "# Get the predicted classes\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map the classes to risk levels\n",
    "risk_mapping = {0: 'Low Risk', 1: 'Medium Risk', 2: 'High Risk'}\n",
    "predicted_risks = [risk_mapping[cls] for cls in predicted_classes]\n",
    "\n",
    "# Print the predicted risk levels for each sample\n",
    "for i, risk in enumerate(predicted_risks):\n",
    "    print(f'Sample {i+1}: Predicted Risk Level - {risk}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033274e-a363-4981-82f0-315ff1e989e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
